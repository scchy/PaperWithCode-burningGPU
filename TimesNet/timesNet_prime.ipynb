{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import fft \n",
    "from Embed import DataEmbedding\n",
    "from Conv_Blocks import Inception_Block_V1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from timesfeatures import time_features\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "import time \n",
    "from timesNet import Model as timesNetModel\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. loading data \n",
    "- long term forcast\n",
    "- data: [ETT-small](https://github.com/zhouhaoyi/ETDataset/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17420, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>5.827</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.462</td>\n",
       "      <td>4.203</td>\n",
       "      <td>1.340</td>\n",
       "      <td>30.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>5.693</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.142</td>\n",
       "      <td>1.371</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 02:00:00</td>\n",
       "      <td>5.157</td>\n",
       "      <td>1.741</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.355</td>\n",
       "      <td>3.777</td>\n",
       "      <td>1.218</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 03:00:00</td>\n",
       "      <td>5.090</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.391</td>\n",
       "      <td>3.807</td>\n",
       "      <td>1.279</td>\n",
       "      <td>25.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 04:00:00</td>\n",
       "      <td>5.358</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.462</td>\n",
       "      <td>3.868</td>\n",
       "      <td>1.279</td>\n",
       "      <td>21.948000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date   HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
       "0  2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
       "1  2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
       "2  2016-07-01 02:00:00  5.157  1.741  1.279  0.355  3.777  1.218  27.787001\n",
       "3  2016-07-01 03:00:00  5.090  1.942  1.279  0.391  3.807  1.279  25.044001\n",
       "4  2016-07-01 04:00:00  5.358  1.942  1.492  0.462  3.868  1.279  21.948000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"/home/scc/Downloads/Datas/ETDataset/ETT-small\"\n",
    "data_file = f'{data_dir}{os.sep}ETTh1.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 8640), (8544, 11520), (11424, 14400)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_ = 30 * 24\n",
    "seq_len = 96\n",
    "border1s = [0, 12 * base_ - seq_len, (12 + 4) * base_ - seq_len]\n",
    "border2s = [12 * base_, (12 + 4) * base_, (12 + 8) * base_]\n",
    "list(zip(border1s, border2s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataSet & dataLoader\n",
    "\n",
    "- dataset: [Time-Series-Library:data_provider/data_loader.py:Dataset_ETT_hour](https://github.com/thuml/Time-Series-Library/blob/main/data_provider/data_loader.py)\n",
    "- dataLoader: [Time-Series-Library:data_provider/data_factory.py:data_provider](https://github.com/thuml/Time-Series-Library/blob/main/data_provider/data_factory.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_ETT_hour(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=[24 * 4 * 4, 24 * 4, 24 *4], features='S', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='h', seasonal_patterns=None):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        self.seq_len = size[0]\n",
    "        self.label_len = size[1]\n",
    "        self.pred_len = size[2]\n",
    "        assert flag in ['train', 'val', 'test']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "        \n",
    "        self.features = features\n",
    "        self.target = target \n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq \n",
    "        \n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "        \n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(f'{self.root_path}{os.sep}{self.data_path}')\n",
    "        base_ = 30 * 24\n",
    "        border1s = [0, 12 * base_ - self.seq_len, (12 + 4) * base_ - self.seq_len]\n",
    "        border2s = [12 * base_, (12 + 4) * base_, (12 + 8) * base_]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "        print(self.set_type, f\"{border1} -> {border2}\")\n",
    "        if self.features in ['M', 'MS']: \n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "        \n",
    "        print(f\"df_raw.shape={df_raw.shape} data.shape={data.shape}\")\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        if self.timeenc == 1: # encoded as value between [-0.5, 0.5]\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "        \n",
    "        \n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        print(\"data.shape=\", data.shape, \"\\nself.data_x.shape=\", self.data_x.shape)\n",
    "        self.data_stamp = data_stamp\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        \n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_data_provider(args, flag):\n",
    "    Data = Dataset_ETT_hour\n",
    "    timeenc = 0 if args.embed != 'timeF' else 1\n",
    "    drop_last = True\n",
    "    shuffle_flag = True\n",
    "    freq = args.freq\n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        batch_size = 1\n",
    "    else:\n",
    "        batch_size = args.batch_size\n",
    "\n",
    "    data_set = Data(\n",
    "        root_path=args.root_path,\n",
    "        data_path=args.data_path,\n",
    "        flag=flag,\n",
    "        size=[args.seq_len, args.label_len, args.pred_len],\n",
    "        features=args.features,\n",
    "        target=args.target,\n",
    "        timeenc=timeenc,\n",
    "        freq=freq,\n",
    "        seasonal_patterns=args.seasonal_patterns\n",
    "    )\n",
    "    print(flag, len(data_set))\n",
    "    data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_flag,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=drop_last)\n",
    "    return data_set, data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def vali(args, model, device, vali_data, vali_loader, criterion):\n",
    "    total_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float()\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # decoder input\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            # encoder - decoder\n",
    "            if args.use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if args.output_attention:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            else:\n",
    "                if args.output_attention:\n",
    "                    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                else:\n",
    "                    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:].to(device)\n",
    "\n",
    "            pred = outputs.detach().cpu()\n",
    "            true = batch_y.detach().cpu()\n",
    "\n",
    "            loss = criterion(pred, true)\n",
    "\n",
    "            total_loss.append(loss)\n",
    "    total_loss = np.average(total_loss)\n",
    "    model.train()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
    "    if args.lradj == 'type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "    elif args.lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "\n",
    "def train(model, args, setting):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    train_data, train_loader = simple_data_provider(args, flag='train')\n",
    "    vali_data, vali_loader = simple_data_provider(args, flag='val')\n",
    "    test_data, test_loader = simple_data_provider(args, flag='test') \n",
    "\n",
    "    path = os.path.join(args.checkpoints, setting)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    time_now = time.time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "\n",
    "    model_optim = Adam(model.parameters(), lr=args.learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if args.use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(args.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        epoch_time = time.time()\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "            batch_x = batch_x.float().to(device)\n",
    "\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # decoder input\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "\n",
    "            # encoder - decoder\n",
    "            if args.use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if args.output_attention:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                    f_dim = -1 if args.features == 'MS' else 0\n",
    "                    outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "                    batch_y = batch_y[:, -args.pred_len:, f_dim:].to(device)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    train_loss.append(loss.item())\n",
    "            else:\n",
    "                if args.output_attention:\n",
    "                    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                else:\n",
    "                    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                f_dim = -1 if args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -args.pred_len:, f_dim:].to(device)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                speed = (time.time() - time_now) / iter_count\n",
    "                left_time = speed * ((args.train_epochs - epoch) * train_steps - i)\n",
    "                print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time.time()\n",
    "\n",
    "            if args.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(model_optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                model_optim.step()\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        vali_loss = vali(args, model, device, vali_data, vali_loader, criterion)\n",
    "        test_loss = vali(args, model, device, test_data, test_loader, criterion)\n",
    "\n",
    "        print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "            epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "        early_stopping(vali_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        adjust_learning_rate(model_optim, epoch + 1, args)\n",
    "\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    task_name='long_term_forecast' ,\n",
    "    is_training=1 ,\n",
    "    root_path=data_dir,\n",
    "    data_path='ETTh1.csv',\n",
    "    model_id='ETTh1_96_336' ,\n",
    "    model='TimesNet',\n",
    "    data='ETTh1' ,\n",
    "    features='M' ,\n",
    "    seq_len=96 ,\n",
    "    label_len=48 ,\n",
    "    pred_len=96 ,\n",
    "    e_layers=2 ,\n",
    "    d_layers=1 ,\n",
    "    factor=3 ,\n",
    "    enc_in=7 ,\n",
    "    dec_in=7 ,\n",
    "    c_out=7 ,\n",
    "    n_heads=8,\n",
    "    d_model=16 ,\n",
    "    embed='timeF',\n",
    "    d_ff=32 ,\n",
    "    des='Exp' ,\n",
    "    distil=True,\n",
    "    itr=1 ,\n",
    "    num_kernels=6,\n",
    "    top_k=5,\n",
    "    freq='h',\n",
    "    dropout=0.1,\n",
    "    batch_size=32,\n",
    "    target='OT',\n",
    "    seasonal_patterns=\"Monthly\",\n",
    "    num_workers=10,\n",
    "    checkpoints='./checkpoints/',\n",
    "    patience=7,\n",
    "    learning_rate=0.0001,\n",
    "    use_amp=False,\n",
    "    train_epochs=10,\n",
    "    output_attention=False,\n",
    "    lradj='type1'\n",
    ")\n",
    "\n",
    "setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "                args.task_name,\n",
    "                args.model_id,\n",
    "                args.model,\n",
    "                args.data,\n",
    "                args.features,\n",
    "                args.seq_len,\n",
    "                args.label_len,\n",
    "                args.pred_len,\n",
    "                args.d_model,\n",
    "                args.n_heads,\n",
    "                args.e_layers,\n",
    "                args.d_layers,\n",
    "                args.d_ff,\n",
    "                args.factor,\n",
    "                args.embed,\n",
    "                args.distil,\n",
    "                args.des, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -> 8640\n",
      "df_raw.shape=(17420, 8) data.shape=(17420, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape= (17420, 7) \n",
      "self.data_x.shape= (8640, 7)\n",
      "train 8449\n",
      "1 8544 -> 11520\n",
      "df_raw.shape=(17420, 8) data.shape=(17420, 7)\n",
      "data.shape= (17420, 7) \n",
      "self.data_x.shape= (2976, 7)\n",
      "val 2785\n",
      "2 11424 -> 14400\n",
      "df_raw.shape=(17420, 8) data.shape=(17420, 7)\n",
      "data.shape= (17420, 7) \n",
      "self.data_x.shape= (2976, 7)\n",
      "test 2785\n",
      "\titers: 100, epoch: 1 | loss: 0.5475962\n",
      "\tspeed: 0.0449s/iter; left time: 114.0182s\n",
      "\titers: 200, epoch: 1 | loss: 0.4669370\n",
      "\tspeed: 0.0319s/iter; left time: 77.7779s\n",
      "Epoch: 1 cost time: 9.265677452087402\n",
      "Epoch: 1, Steps: 264 | Train Loss: 0.4885100 Vali Loss: 0.8533527 Test Loss: 0.4471019\n",
      "Validation loss decreased (inf --> 0.853353).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3565480\n",
      "\tspeed: 0.2424s/iter; left time: 551.9603s\n",
      "\titers: 200, epoch: 2 | loss: 0.3541487\n",
      "\tspeed: 0.0297s/iter; left time: 64.7493s\n",
      "Epoch: 2 cost time: 7.9857823848724365\n",
      "Epoch: 2, Steps: 264 | Train Loss: 0.4009555 Vali Loss: 0.7987773 Test Loss: 0.4295301\n",
      "Validation loss decreased (0.853353 --> 0.798777).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3596587\n",
      "\tspeed: 0.2469s/iter; left time: 496.9431s\n",
      "\titers: 200, epoch: 3 | loss: 0.3371003\n",
      "\tspeed: 0.0293s/iter; left time: 56.0937s\n",
      "Epoch: 3 cost time: 7.949188947677612\n",
      "Epoch: 3, Steps: 264 | Train Loss: 0.3795679 Vali Loss: 0.7811201 Test Loss: 0.4253980\n",
      "Validation loss decreased (0.798777 --> 0.781120).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3133552\n",
      "\tspeed: 0.2432s/iter; left time: 425.3008s\n",
      "\titers: 200, epoch: 4 | loss: 0.4143973\n",
      "\tspeed: 0.0292s/iter; left time: 48.1739s\n",
      "Epoch: 4 cost time: 7.935633182525635\n",
      "Epoch: 4, Steps: 264 | Train Loss: 0.3712565 Vali Loss: 0.7726180 Test Loss: 0.4244759\n",
      "Validation loss decreased (0.781120 --> 0.772618).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3685763\n",
      "\tspeed: 0.2435s/iter; left time: 361.5899s\n",
      "\titers: 200, epoch: 5 | loss: 0.3600520\n",
      "\tspeed: 0.0292s/iter; left time: 40.3921s\n",
      "Epoch: 5 cost time: 7.923535108566284\n",
      "Epoch: 5, Steps: 264 | Train Loss: 0.3673180 Vali Loss: 0.7696661 Test Loss: 0.4239995\n",
      "Validation loss decreased (0.772618 --> 0.769666).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.3876747\n",
      "\tspeed: 0.2438s/iter; left time: 297.7255s\n",
      "\titers: 200, epoch: 6 | loss: 0.4098744\n",
      "\tspeed: 0.0294s/iter; left time: 32.9209s\n",
      "Epoch: 6 cost time: 8.055259466171265\n",
      "Epoch: 6, Steps: 264 | Train Loss: 0.3653651 Vali Loss: 0.7680885 Test Loss: 0.4239030\n",
      "Validation loss decreased (0.769666 --> 0.768089).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.3608540\n",
      "\tspeed: 0.2462s/iter; left time: 235.5676s\n",
      "\titers: 200, epoch: 7 | loss: 0.3250539\n",
      "\tspeed: 0.0301s/iter; left time: 25.7859s\n",
      "Epoch: 7 cost time: 8.05059552192688\n",
      "Epoch: 7, Steps: 264 | Train Loss: 0.3644432 Vali Loss: 0.7672491 Test Loss: 0.4229458\n",
      "Validation loss decreased (0.768089 --> 0.767249).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.3889826\n",
      "\tspeed: 0.2433s/iter; left time: 168.6269s\n",
      "\titers: 200, epoch: 8 | loss: 0.3299024\n",
      "\tspeed: 0.0292s/iter; left time: 17.3286s\n",
      "Epoch: 8 cost time: 7.936904191970825\n",
      "Epoch: 8, Steps: 264 | Train Loss: 0.3639978 Vali Loss: 0.7668291 Test Loss: 0.4225513\n",
      "Validation loss decreased (0.767249 --> 0.766829).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.4282869\n",
      "\tspeed: 0.2458s/iter; left time: 105.4335s\n",
      "\titers: 200, epoch: 9 | loss: 0.3704366\n",
      "\tspeed: 0.0292s/iter; left time: 9.5973s\n",
      "Epoch: 9 cost time: 7.980992794036865\n",
      "Epoch: 9, Steps: 264 | Train Loss: 0.3638029 Vali Loss: 0.7661443 Test Loss: 0.4225281\n",
      "Validation loss decreased (0.766829 --> 0.766144).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.4039476\n",
      "\tspeed: 0.2531s/iter; left time: 41.7623s\n",
      "\titers: 200, epoch: 10 | loss: 0.3215350\n",
      "\tspeed: 0.0335s/iter; left time: 2.1747s\n",
      "Epoch: 10 cost time: 9.104768991470337\n",
      "Epoch: 10, Steps: 264 | Train Loss: 0.3637726 Vali Loss: 0.7664564 Test Loss: 0.4224430\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Updating learning rate to 1.953125e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): ModuleList(\n",
       "    (0-1): 2 x TimesBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Inception_Block_V1(\n",
       "          (kernels): ModuleList(\n",
       "            (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (3): Conv2d(16, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "            (4): Conv2d(16, 32, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "            (5): Conv2d(16, 32, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "          )\n",
       "        )\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Inception_Block_V1(\n",
       "          (kernels): ModuleList(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (2): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (3): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "            (4): Conv2d(32, 16, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "            (5): Conv2d(32, 16, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (enc_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(7, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=4, out_features=16, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (predict_linear): Linear(in_features=96, out_features=192, bias=True)\n",
       "  (projection): Linear(in_features=16, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = timesNetModel(args)\n",
    "train(model_, args, setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech2text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
